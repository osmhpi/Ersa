{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Code Retrieval using Embeddings\n",
    "\n",
    "## Load Embedding Model\n",
    "\n",
    "We use [CodeT5+](https://huggingface.co/Salesforce/codet5p-110m-embedding) as embedding model.\n",
    "\n",
    "* Maximum input: 512 tokens\n",
    "* Output dimensions: 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.device('cuda:0')\n",
    "model_id = \"Salesforce/codet5p-110m-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id, device_map=gpu, torch_dtype=torch.bfloat16, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(code):\n",
    "    \"\"\"Use the model to embed the given code\"\"\"\n",
    "    inputs = tokenizer.encode(code, return_tensors='pt').to(gpu)\n",
    "    output = model(inputs)[0]\n",
    "    \n",
    "    # convert to \"CPU-friendly\" datatype (GPU has bfloat16, which is incompatible with x86 code) and pull to CPU\n",
    "    return output.detach().to(torch.float32).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out an example embedding:\n",
    "example_code = '''def fib(n):\n",
    "    return fib(n - 1) + fib(n - 2) if n > 1 else n\n",
    "    '''\n",
    "embed(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## üë©‚Äçüíª Build a Simple Code \"Database\"\n",
    "\n",
    "We need some code to retrieve. For this example, we use all functions in the [Flask](https://github.com/pallets/flask) web framework, extracting them using GitPyton and tree-sitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import autopep8\n",
    "import tree_sitter_python as tspython\n",
    "from tree_sitter import Language, Parser\n",
    "import numpy as np\n",
    "\n",
    "PY_LANGUAGE = Language(tspython.language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repo_to_functions(repo_location):\n",
    "    \"\"\"Extract all Python function definitions from the given Git repository, identifying each by <repo>:<file_name>:<function_name>\"\"\"\n",
    "\n",
    "    # fetch top commit's tree\n",
    "    repo = git.Repo(repo_location)\n",
    "    tree = repo.head.commit.tree\n",
    "\n",
    "    # parser and query for Python\n",
    "    parser = Parser()\n",
    "    parser.language = PY_LANGUAGE\n",
    "    query = PY_LANGUAGE.query('''(function_definition) @func''')\n",
    "\n",
    "    # read all .py files\n",
    "    files = [(item.name, item.data_stream.read())\n",
    "             for item in tree.list_traverse()\n",
    "             if item.type == 'blob'\n",
    "             and item.name.endswith('.py')]\n",
    "\n",
    "    def function_name(node):\n",
    "        return node.child_by_field_name('name').text.decode('utf-8')\n",
    "    \n",
    "    # query all functions in all files. We use <repo>:<file_name>:<function_name> as \"ID\"\n",
    "    functions = {f'{repo_location}:{name}:{function_name(node)}' : autopep8.fix_code(node.text)\n",
    "                 for name, file in files\n",
    "                 for node, _ in query.captures(parser.parse(file).root_node)}\n",
    "\n",
    "    return functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --bare https://github.com/pallets/flask.git\n",
    "\n",
    "# related repository suggestions:\n",
    "# - https://github.com/pallets/werkzeug.git\n",
    "# - https://github.com/pallets/jinja.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# build a dictionary of functions\n",
    "functions = repo_to_functions('./flask.git')\n",
    "# add other repositories:\n",
    "#functions.update(repo_to_functions('<OTHER REPO>'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(functions)} functions extracted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## üìñ Populate a Simple \"Vector Database\"\n",
    "\n",
    "* We compute the code embedding for each function, storing it under the key (function name).\n",
    "* The retriever embeds the query and ranks each item according to cosine similarity.\n",
    "* ‚ö†Ô∏è For larger databases, use a real vector database or specialized frameworks (e.g., [LlamaIndex](https://docs.llamaindex.ai/en/stable/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# compute embeddings for each function\n",
    "embeddings = {name : embed(code) for name, code in functions.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(embeddings, query):\n",
    "    \"\"\"Rank embedded items by their similarity to the query\"\"\"\n",
    "    \n",
    "    query_embed = embed(query)\n",
    "    similarities = [(name, np.dot(query_embed, embedding))\n",
    "                    for name, embedding in embeddings.items()]\n",
    "    return sorted(similarities, key=lambda item: item[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = retrieve(embeddings, \"# test whether a user can log in\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top 10 results:\n",
    "def print_retrieval_results(ranked_results):\n",
    "    for index, (name, similarity) in enumerate(ranked_results):\n",
    "        print('=' * 80)\n",
    "        print(f'{index + 1}: {name} ({similarity:.2f})')\n",
    "        print('-' * 80)\n",
    "        print(functions[name])\n",
    "print_retrieval_results(top_10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Visualize Embedding Space\n",
    "\n",
    "We map each 256-dimensional vector to a 2D vector using PCA and plot the result.\n",
    "* Tests are red\n",
    "* Non-test functions are blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)\n",
    "keys = list(embeddings.keys())\n",
    "vectors = list(embeddings.values())\n",
    "\n",
    "# do the PCA transformation\n",
    "projected = pca.fit_transform(vectors)\n",
    "\n",
    "# define a color for each data point based on the keys\n",
    "colors = ['red' if 'test_' in k else 'blue' for k in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = projected[:, 0], y=projected[:,1], color=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Generation-augmented Retrieval (GAR)\n",
    "\n",
    "* Generation-augmented retrieval expands the user's query using an LLM.\n",
    "* Retrieval compares against the embedding of the generated query\n",
    "* Here, we do **code completion** to obtain an example code from **natural language**, which should make it **easier to match against other code** because they now share the same qualities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GemmaTokenizer, AutoModelForCausalLM\n",
    "gen_model_id = \"google/codegemma-1.1-2b\"\n",
    "gen_tokenizer = GemmaTokenizer.from_pretrained(gen_model_id)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_id, device_map=gpu, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A standard greedy generation helper\n",
    "def generate(prompt, max_new_tokens=128):\n",
    "    inputs = gen_tokenizer.encode(prompt, return_tensors='pt').to(gpu)\n",
    "    outputs = gen_model.generate(inputs, max_new_tokens=max_new_tokens)\n",
    "    return gen_tokenizer.decode(outputs[0])\n",
    "\n",
    "def generate_retrieve(embeddings, prompt):\n",
    "    generated = generate(prompt)\n",
    "    return retrieve(embeddings, generated), generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10, generated = generate_retrieve(embeddings, \"# A flask test case to verify user login functionality:\\n\")[:10]\n",
    "\n",
    "print('Generated query:')\n",
    "print(generated)\n",
    "\n",
    "print_retrieval_results(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
